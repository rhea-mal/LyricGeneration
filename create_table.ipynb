{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "#dataset = load_dataset(\"maharshipandya/spotify-tracks-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset combined and saved.\n"
     ]
    }
   ],
   "source": [
    "lyrics1_path = 'lyrics1_dataset.csv'\n",
    "lyrics2_folder_path = 'lyrics2_dataset'\n",
    "\n",
    "no_lyrics_text = \"lyrics for this song have yet to be released please check back once the song has been released\"\n",
    "\n",
    "# Read the first dataset\n",
    "df_master = pd.read_csv(lyrics1_path, usecols=['Artist Name', 'Song Name', 'Lyrics'])\n",
    "df_master.columns = ['Artist', 'Title', 'Lyrics']  # Standardize column names\n",
    "\n",
    "# Iterate through each CSV file in the lyrics2_dataset folder\n",
    "for file in os.listdir(lyrics2_folder_path):\n",
    "    file_path = os.path.join(lyrics2_folder_path, file)\n",
    "    if os.path.isfile(file_path) and file_path.endswith('.csv'):\n",
    "        # Read the dataset\n",
    "        df_temp = pd.read_csv(file_path, usecols=['Artist', 'Title', 'Lyric'])\n",
    "\n",
    "        # Filter out rows with no lyrics\n",
    "        df_temp = df_temp.loc[~df_temp['Lyric'].str.contains(no_lyrics_text, case=False, na=False)]\n",
    "        \n",
    "        df_temp.columns = ['Artist', 'Title', 'Lyrics']  # Standardize column names\n",
    "        \n",
    "        # Concatenate with the master DataFrame\n",
    "        df_master = pd.concat([df_master, df_temp], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "allowed_chars_pattern = re.compile(r'[^a-zA-Z0-9 .,;\\'\"?!()-]')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace('\"', '')\n",
    "    cleaned_text = re.sub(allowed_chars_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(str).apply(clean_text)\n",
    "    return df\n",
    "\n",
    "# Assuming df_master is your DataFrame\n",
    "df_master_cleaned = clean_dataframe(df_master)\n",
    "\n",
    "df_master.to_csv('combined_lyrics_dataset.csv', index=False)\n",
    "\n",
    "print(\"Dataset combined and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('features_dataset.csv').drop_duplicates(subset=['artists', 'track_name'], keep='first')\n",
    "combined_lyrics_df = df_master\n",
    "features_df['artists'] = features_df['artists'].str.split(';')\n",
    "features_exploded_df = features_df.explode('artists')\n",
    "\n",
    "#merged_df = pd.merge(combined_lyrics_df, features_exploded_df, left_on=['Artist', 'Title'], right_on=['artists', 'track_name'])\n",
    "merged_df = pd.merge(combined_lyrics_df, features_exploded_df, left_on=['Title'], right_on=['track_name'])\n",
    "\n",
    "final_columns = ['Artist', 'Title', 'Lyrics', 'track_id', 'popularity', 'duration_ms', 'explicit', \n",
    "                 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', \n",
    "                 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'track_genre']\n",
    "final_merged_df = merged_df[final_columns]\n",
    "\n",
    "\n",
    "final_merged_df = final_merged_df.drop_duplicates(subset=['Artist', 'Title'], keep='first')\n",
    "final_merged_df.to_csv('final_lyrics_features_combined.csv', index=False)\n",
    "state1 = final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_CHAR = u\"\\u25A1\"  # the empty square character\n",
    "\n",
    "# Function to pad each column\n",
    "def pad_column(series):\n",
    "    max_length = series.astype(str).apply(len).max()  # Find the max length in the column\n",
    "    return series.astype(str).apply(lambda x: x + (PAD_CHAR * (max_length - len(x))))  # Pad each element\n",
    "\n",
    "# Apply padding to each column and then concatenate\n",
    "columns_to_concatenate = ['Artist', 'Title', 'popularity', 'duration_ms', 'explicit', \n",
    "                          'danceability', 'energy', 'key', 'loudness', 'mode', \n",
    "                          'speechiness', 'acousticness', 'instrumentalness', \n",
    "                          'liveness', 'valence', 'tempo', 'time_signature', \n",
    "                          'track_genre']\n",
    "\n",
    "# Apply padding\n",
    "for col in columns_to_concatenate:\n",
    "    final_merged_df[col] = pad_column(final_merged_df[col])\n",
    "\n",
    "# Concatenate the padded columns\n",
    "final_merged_df['metadata'] = final_merged_df[columns_to_concatenate].agg(' '.join, axis=1)\n",
    "\n",
    "# Select the 'metadata' and 'lyrics' columns to form a new DataFrame\n",
    "new_df = final_merged_df[['metadata', 'Lyrics']].drop_duplicates(subset=['Lyrics'], keep='first')\n",
    "# Output the new DataFrame to a TSV file\n",
    "new_df.to_csv('metadata.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in total 1127 rows\n",
    "# shuffle rows\n",
    "shuffled_df = new_df.sample(frac=1).reset_index(drop=True)\n",
    "# make lyrics_dev.tsv first 300\n",
    "shuffled_df[:300].to_csv('lyrics_dev.tsv', sep='\\t', index=False)\n",
    "\n",
    "# make lyrics_train.tsv last 827 random values\n",
    "shuffled_df[300:].to_csv('lyrics_train.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluctuated popularity within range (-10, 10).\n",
      "Fluctuated duration_ms within range (-300, 300).\n",
      "Fluctuated danceability within range (-0.2, 0.2).\n",
      "Fluctuated energy within range (-0.2, 0.2).\n",
      "Fluctuated speechiness within range (-0.1, 0.1).\n",
      "Fluctuated acousticness within range (-0.1, 0.1).\n",
      "Fluctuated instrumentalness within range (-0.1, 0.1).\n",
      "Fluctuated liveness within range (-0.1, 0.1).\n",
      "Padded Artist.\n",
      "Padded Title.\n",
      "Padded popularity.\n",
      "Padded duration_ms.\n",
      "Padded explicit.\n",
      "Padded danceability.\n",
      "Padded energy.\n",
      "Padded key.\n",
      "Padded loudness.\n",
      "Padded mode.\n",
      "Padded speechiness.\n",
      "Padded acousticness.\n",
      "Padded instrumentalness.\n",
      "Padded liveness.\n",
      "Padded valence.\n",
      "Padded tempo.\n",
      "Padded time_signature.\n",
      "Padded track_genre.\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL 3 AGAIN before running this.\n",
    "# make lyrics_inputs.tsv 100 --> randomly shuffle the columns\n",
    "shuffled_df = state1.apply(lambda x: x.sample(frac=1).reset_index(drop=True) if x.name != 'lyrics' else x)\n",
    "\n",
    "# Define fluctuation ranges\n",
    "fluctuation_ranges = {\n",
    "    'popularity': (-10, 10),\n",
    "    'duration_ms': (-300, 300),\n",
    "    'danceability': (-0.2, 0.2),\n",
    "    'energy': (-0.2, 0.2),\n",
    "    'speechiness': (-0.1, 0.1),\n",
    "    'acousticness': (-0.1, 0.1),\n",
    "    'instrumentalness': (-0.1, 0.1),\n",
    "    'liveness': (-0.1, 0.1)\n",
    "}\n",
    "\n",
    "# Apply fluctuations\n",
    "for column, fluct_range in fluctuation_ranges.items():\n",
    "    if column in shuffled_df.columns:\n",
    "        random_fluctuation = np.random.uniform(fluct_range[0], fluct_range[1], shuffled_df.shape[0])\n",
    "        shuffled_df[column] = shuffled_df[column].astype(float) + random_fluctuation\n",
    "        print(f\"Fluctuated {column} within range {fluct_range}.\")\n",
    "\n",
    "# Padding\n",
    "PAD_CHAR = u\"\\u25A1\"  # Define padding character\n",
    "\n",
    "columns_to_concatenate = ['Artist', 'Title', 'popularity', 'duration_ms', 'explicit', \n",
    "                          'danceability', 'energy', 'key', 'loudness', 'mode', \n",
    "                          'speechiness', 'acousticness', 'instrumentalness', \n",
    "                          'liveness', 'valence', 'tempo', 'time_signature', \n",
    "                          'track_genre']\n",
    "\n",
    "shuffled_df['explicit'] = shuffled_df['explicit'].astype(str)\n",
    "state1['explicit'] = state1['explicit'].map({1: 'True', 0: 'False'})\n",
    "\n",
    "# Assuming pad_column function is defined as in previous instructions\n",
    "for col in columns_to_concatenate:\n",
    "    if col in shuffled_df.columns:\n",
    "        shuffled_df[col] = pad_column(shuffled_df[col])\n",
    "        print(f\"Padded {col}.\")\n",
    "\n",
    "# Create a new DataFrame for metadata\n",
    "shuffled_df['metadata'] = shuffled_df[columns_to_concatenate].agg(' '.join, axis=1)\n",
    "new_df = shuffled_df[['metadata', 'Lyrics']]  # Assuming 'lyrics' is a column to be preserved as is\n",
    "\n",
    "new_df[:100].to_csv('lyrics_inputs.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file written to: cleaned_lyrics_dev.tsv\n",
      "Cleaned file written to: cleaned_lyrics_inputs.tsv\n",
      "Cleaned file written to: cleaned_lyrics_train.tsv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the pad character\n",
    "PAD_CHAR = u\"\\u25A1\"\n",
    "\n",
    "# Define the regex pattern for allowed characters\n",
    "allowed_chars_pattern = re.compile(r\"[^a-zA-Z0-9.,'?! \\u25A1]\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text by removing any characters not allowed.\n",
    "    \"\"\"\n",
    "    # Replace characters not in the allowed list with an empty string\n",
    "    cleaned_text = re.sub(allowed_chars_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_tsv(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Clean the TSV file by removing the first row and any disallowed characters.\n",
    "    \"\"\"\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in file:\n",
    "            cleaned_line = clean_text(line)\n",
    "            outfile.write(cleaned_line)\n",
    "\n",
    "# List of TSV files to clean\n",
    "tsv_files = ['lyrics_dev.tsv', 'lyrics_inputs.tsv', 'lyrics_train.tsv']\n",
    "\n",
    "# Clean each TSV file\n",
    "for file_name in tsv_files:\n",
    "    # Define the output file name, you can adjust the naming convention as needed\n",
    "    output_file_name = f\"cleaned_{file_name}\"\n",
    "    clean_tsv(file_name, output_file_name)\n",
    "    print(f\"Cleaned file written to: {output_file_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
