{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "C9ie33zD6orX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhnvebPM6Rm0",
        "outputId": "7e1f765a-39e5-4050-f5f6-376a54002b54"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nnl-uepJixFi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm  # For progress bars\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sacrebleu.metrics import BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_gdn3wrixFj",
        "outputId": "eecf49a8-769d-4846-aa19-ab13b019cd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5591fc70e5f8>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  categorical_df[['Before_Tab', 'After_Tab']] = categorical_df['Genre'].str.split('\\t', n=1, expand=True)\n",
            "<ipython-input-13-5591fc70e5f8>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  categorical_df[['Before_Tab', 'After_Tab']] = categorical_df['Genre'].str.split('\\t', n=1, expand=True)\n",
            "<ipython-input-13-5591fc70e5f8>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  categorical_df.drop(columns=['Lyrics'], inplace=True)\n",
            "<ipython-input-13-5591fc70e5f8>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  categorical_df.drop(columns=['Genre'], inplace=True)\n"
          ]
        }
      ],
      "source": [
        "file_path = 'lyrics_train.tsv'\n",
        "file_path_2 = 'lyrics_dev.tsv'\n",
        "\n",
        "# Adjusted to load the dataset with ' | ' separation\n",
        "df_first = pd.read_csv(file_path, sep=' \\| ', engine='python', header=None, names=['Artist', 'Title', 'Popularity', 'Duration_ms', 'Explicit', 'Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo', 'Time_Signature', 'Track_Genre', 'Lyrics'])\n",
        "df_second = pd.read_csv(file_path_2, sep=' \\| ', engine='python', header=None, names=['Artist', 'Title', 'Popularity', 'Duration_ms', 'Explicit', 'Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo', 'Time_Signature', 'Track_Genre', 'Lyrics'])\n",
        "\n",
        "df_first = df_first.replace('□', '', regex=True)\n",
        "df_second = df_second.replace('□', '', regex=True)\n",
        "\n",
        "df = pd.concat([df_first, df_second], axis=0)\n",
        "df = df[~df['Artist'].str.contains('BTS')]\n",
        "df['Lyrics'] = df['Lyrics'].apply(lambda x: x.replace('   ', '\\n') if isinstance(x, str) else x)\n",
        "\n",
        "df.drop(columns=['Time_Signature'], inplace=True)\n",
        "df.drop(columns=['Key'], inplace=True)\n",
        "df.drop(columns=['Mode'], inplace=True)\n",
        "df = df.rename(columns={'Duration_ms': 'Duration'})\n",
        "df = df.rename(columns={'Track_Genre': 'Genre'})\n",
        "df['Explicit'] = df['Explicit'].replace({'True': 1, 'False': 0})\n",
        "\n",
        "# SETUP CATEGORICAL VS NUMERICAL FEATURES\n",
        "categorical_columns = ['Artist', 'Title', 'Genre', 'Lyrics']\n",
        "numerical_columns = ['Popularity', 'Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo']\n",
        "\n",
        "categorical_df = df[categorical_columns]\n",
        "numerical_df = df[numerical_columns]\n",
        "categorical_df[['Before_Tab', 'After_Tab']] = categorical_df['Genre'].str.split('\\t', n=1, expand=True)\n",
        "categorical_df.drop(columns=['Lyrics'], inplace=True)\n",
        "categorical_df.drop(columns=['Genre'], inplace=True)\n",
        "\n",
        "categorical_df = categorical_df.rename(columns={'Before_Tab': 'Genre'})\n",
        "categorical_df = categorical_df.rename(columns={'After_Tab': 'Lyrics'})\n",
        "categorical_df['Lyrics'] = categorical_df['Lyrics'].str.strip()\n",
        "categorical_df['Lyrics'] = categorical_df['Lyrics'].fillna('').astype(str)\n",
        "\n",
        "complete_df = pd.concat([categorical_df, numerical_df], axis=1)\n",
        "\n",
        "print(len(complete_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKbtkpyhixFk",
        "outputId": "877c4e39-b803-40fd-c642-fa2f5e1c5528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad token: [PAD] ID: 50257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50258, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Add [PAD] token to the tokenizer and update pad_token_id\n",
        "pad_token = '[PAD]'\n",
        "if pad_token not in tokenizer.get_added_vocab():\n",
        "    tokenizer.add_special_tokens({'pad_token': pad_token})\n",
        "\n",
        "# Check if the pad_token is recognized correctly\n",
        "print(\"Pad token:\", tokenizer.pad_token, \"ID:\", tokenizer.pad_token_id)\n",
        "\n",
        "# Initialize the model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "# Resize model embeddings to account for the new token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "# IMPORTANT: Set the model's pad_token_id to match the tokenizer's pad_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2ryA4g9nixFk"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for idx, row in complete_df.iterrows():\n",
        "    features_text = ' '.join([f\"|Artist| {row['Artist']}\", f\"|Title| {row['Title']}\", f\"|Genre| {row['Genre']}\"] +\n",
        "                             [f\"|{feature}| {row[feature]}\" for feature in numerical_columns])\n",
        "    input_sequence = f\"{features_text} |Lyrics| {row['Lyrics']}\"\n",
        "    input_sequences.append(input_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CGqciW33ixFk"
      },
      "outputs": [],
      "source": [
        "# Batch tokenize all input sequences, apply padding, and generate attention masks\n",
        "inputs = tokenizer(input_sequences,\n",
        "                   padding=True,  # Pad to the longest sequence\n",
        "                   truncation=True,  # Truncate to max model length\n",
        "                   return_tensors=\"pt\",  # Return PyTorch tensors\n",
        "                   max_length=512)  # Max length for truncation\n",
        "\n",
        "# Extract padded token IDs and attention masks\n",
        "input_ids = inputs['input_ids']\n",
        "attention_masks = inputs['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6WojPnkSixFk"
      },
      "outputs": [],
      "source": [
        "# Wrap input_ids and attention_masks in a TensorDataset\n",
        "dataset = TensorDataset(input_ids, attention_masks)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_Ca3t9xixFk",
        "outputId": "d9f50c43-88cc-4353-d4ea-48ec05be42c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "epochs = 5\n",
        "# Calculate the total number of training steps\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Initialize the scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,  # No warm-up\n",
        "                                            num_training_steps=total_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "n070998iixFl",
        "outputId": "d001f2e1-c184-437b-f3a2-e19b4f387d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   1%|          | 1/118 [00:01<03:18,  1.69s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 133.06 MiB is free. Process 9206 has 14.62 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 703.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b14d33f4696d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 133.06 MiB is free. Process 9206 has 14.62 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 703.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
        "        b_input_ids, b_attention_mask = batch\n",
        "        b_input_ids = b_input_ids.to(device)\n",
        "        b_attention_mask = b_attention_mask.to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update the learning rate\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item()}\")\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Validation step with BLEU score calculation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    hypotheses = []  # Generated sequences\n",
        "    references = []  # Actual sequences\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_loader):\n",
        "            b_input_ids, b_attention_mask = batch\n",
        "            b_input_ids = b_input_ids.to(device)\n",
        "            b_attention_mask = b_attention_mask.to(device)\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_input_ids)\n",
        "            loss = outputs.loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Generate predictions\n",
        "            predictions = model.generate(b_input_ids, max_length=512)\n",
        "\n",
        "            # Convert predictions and references to text\n",
        "            hyp_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in predictions]\n",
        "            ref_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in b_input_ids]\n",
        "\n",
        "            hypotheses.extend(hyp_texts)\n",
        "            references.extend([[r] for r in ref_texts])  # BLEU expects a list of references\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_bleu = corpus_bleu(hypotheses, references).score\n",
        "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss}, Validation BLEU: {val_bleu}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = './gpt2_lyrics_model'\n",
        "model.save_pretrained(model_save_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer_save_path = './gpt2_lyrics_tokenizer'\n",
        "tokenizer.save_pretrained(tokenizer_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpdwCvdjl7Ik",
        "outputId": "ade21df9-c46b-4253-aa2c-694903e3f737"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2_lyrics_tokenizer/tokenizer_config.json',\n",
              " './gpt2_lyrics_tokenizer/special_tokens_map.json',\n",
              " './gpt2_lyrics_tokenizer/vocab.json',\n",
              " './gpt2_lyrics_tokenizer/merges.txt',\n",
              " './gpt2_lyrics_tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_save_path)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Resize the token embeddings in case new tokens were added\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "# IMPORTANT: Set the model's pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Model pad_token_id:\", model.config.pad_token_id)\n",
        "\n",
        "# Make sure to move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acgfzRBZmkOj",
        "outputId": "db86ba5b-675b-4b31-ed6a-ccea84a2b2e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model pad_token_id: 50257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50258, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lyrics(prompt_text, max_length=100):\n",
        "    # Encode the prompt text\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    encoded_prompt = encoded_prompt.to(device)\n",
        "    attention_mask = torch.ones(encoded_prompt.shape, dtype=torch.long, device=device)  # Assuming no actual padding is needed here\n",
        "\n",
        "    # Generate a sequence of tokens following the prompt\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=encoded_prompt,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=100,\n",
        "        temperature=1.0,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens to text\n",
        "    generated_sequence = output_sequences[0].tolist()\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove the prompt text from the output\n",
        "    text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)):]\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Example usage\n",
        "prompt_text = \"The moon shines brightly in the night, \"\n",
        "lyrics = generate_lyrics(prompt_text, max_length=100)\n",
        "print(\"Generated Lyrics:\\n\" + lyrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmdlySv4mq0H",
        "outputId": "f8a83e56-e23f-48d9-8971-9c1a523fcc9f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Lyrics:\n",
            "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD] day[PAD] the top of your hand comes on a black metal[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD] day the[PAD][PAD][PAD][PAD][PAD][PAD][PAD] year you took out some new drugs and we got a new life for that money in this city\n",
            " The[PAD][PAD][PAD][PAD][PAD][PAD] day the top is my way to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"|Artist| Justin Bieber |Genre| Pop |Energy| 0.502 |Tempo| 120 |Lyrics| \"\n",
        "lyrics = generate_lyrics(prompt_text, max_length=100)\n",
        "print(\"Generated Lyrics:\\n\" + lyrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il0o9zF7nHMg",
        "outputId": "c8ea1a8e-2157-409d-a5bf-8f57636428ae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Lyrics:\n",
            "༽_Dance it up please cause i'll never forget a song that's like nothing else you've done oh right now is this what makes me wanna be king do we ever once? I'm sorry when your shit starts to[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD] I thee howst all my life would have been so much better if not for those two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"It's about time we broke up my love\"\n",
        "lyrics = generate_lyrics(prompt_text, max_length=100)\n",
        "print(\"Generated Lyrics:\\n\" + lyrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XusZkySc1VxK",
        "outputId": "fc55968d-c8b3-4d30-ed6d-71e65b4dd54b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Lyrics:\n",
            "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD] the[PAD][PAD][PAD] the the one you[PAD] the[PAD][PAD][PAD][PAD][PAD][PAD][PAD] one the one the[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example thematic keywords related to \"love\" songs\n",
        "thematic_keywords = ['love', 'heart', 'forever', 'together', 'passion']\n",
        "def thematic_keyword_match(lyrics, keywords):\n",
        "    \"\"\"\n",
        "    Counts the number of thematic keywords present in the generated lyrics.\n",
        "    \"\"\"\n",
        "    matches = sum(1 for word in keywords if word in lyrics.lower())\n",
        "    return matches\n",
        "\n",
        "# Example usage with generated lyrics\n",
        "generated_lyrics = \"This love has taken its toll on me, She said goodbye too many times before.\"\n",
        "matches = thematic_keyword_match(generated_lyrics, thematic_keywords)\n",
        "print(f\"Thematic Keywords Found: {matches} out of {len(thematic_keywords)}\")\n",
        "\n",
        "def evaluate_thematic_content(model, tokenizer, dataloader, device, keywords):\n",
        "    model.eval()\n",
        "    keyword_matches = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            generated_outputs = model.generate(b_input_ids, max_length=512)\n",
        "            for output in generated_outputs:\n",
        "                lyrics = tokenizer.decode(output, skip_special_tokens=True)\n",
        "                matches = thematic_keyword_match(lyrics, keywords)\n",
        "                keyword_matches.append(matches)\n",
        "\n",
        "    avg_matches = sum(keyword_matches) / len(keyword_matches)\n",
        "    print(f\"Average Thematic Keywords Found: {avg_matches}\")\n",
        "    return avg_matches\n",
        "\n",
        "# Calculate the average number of thematic keyword matches in the validation set\n",
        "avg_matches = evaluate_thematic_content(model, tokenizer, val_loader, device, thematic_keywords)"
      ],
      "metadata": {
        "id": "3iqXgze-8_qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_losses, 'bo-', label='Training Loss')\n",
        "plt.plot(epochs, val_losses, 'ro-', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f_IpJBMr9gZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, bleu_scores, 'go-', label='Validation BLEU Score')\n",
        "plt.title('BLEU Score Progression Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('BLEU Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M4BTcDVw9ok0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, avg_keyword_matches, 'mo-', label='Average Thematic Keyword Matches')\n",
        "plt.title('Thematic Keyword Match Frequency Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Matches')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uX5yC4Jt9rH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, learning_rates, 'ko-', label='Learning Rate')\n",
        "plt.title('Learning Rate Schedule Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.yscale('log')  # Use logarithmic scale if learning rates vary widely\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S-DnyxmO9sph"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}